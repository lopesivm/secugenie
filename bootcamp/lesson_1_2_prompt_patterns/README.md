# LessonÂ 1.2Â â€” Prompt Patterns ğŸ§©

**WeekÂ 1 â€” LLMÂ AppÂ Bootcamp**

---

## ğŸ¯ Goal

Master the core promptâ€‘engineering patterns youâ€™ll reuse throughout SecuGenie:

1. **Fewâ€‘shot** â†’ leverage examples to steer the model.
2. **Chainâ€‘ofâ€‘Thought (CoT)** â†’ coax stepâ€‘byâ€‘step reasoning.
3. **ReAct** (Reason + Act) â†’ interleave thinking with tool calls.
4. **Selfâ€‘Consistency** â†’ sample multiple thoughts and pick the best.

Youâ€™ll implement these in LangChain 0.3 so they plug straight into later RAG and agent pipelines.

---

## âœ… Milestones & Exercises

| Step | Pattern                     | What youâ€™ll build                                                                                                                            |
| ---- | --------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| 1    | **Fewâ€‘shot Q\&A**           | Use `PromptTemplate` to supply 2 sample CVE Q\&As, then ask a new question. Measure quality vs zeroâ€‘shot.                                    |
| 2    | **CoT Math**                | Prompt GPTâ€‘4oâ€‘mini to solve â€œ(87Â Ã—Â 23)Â /Â 46â€ with `"Letâ€™s think step by step"` and parse the answer.                                         |
| 3    | **ReAct + Calculator Tool** | Implement a simple `calculate(expr)` tool (Python eval) and use LangChainâ€™s ReAct agent to answer â€œWhatâ€™s the square root of (511Â²Â +Â 498Â²)?â€ |
| 4    | **Selfâ€‘Consistency Voting** | Sample 5 CoT traces for â€œWhich year did Heartbleed (CVEâ€‘2014â€‘0160) surface?â€ Aggregate majority answer.                                      |
| 5    | **Benchmark**               | Create `prompt_eval.py` that times each pattern on the same prompt set and prints accuracy / latency.                                        |

---

## ğŸ—‚ï¸ Folder Layout

```bash
bootcamp/lesson_1_2_prompt_patterns/
â”œâ”€â”€ README.md              # â† this file
â”œâ”€â”€ few_shot.py            # ExerciseÂ 1
â”œâ”€â”€ cot_math.py            # ExerciseÂ 2
â”œâ”€â”€ react_calc.py          # ExerciseÂ 3
â”œâ”€â”€ self_consistency.py    # ExerciseÂ 4
â””â”€â”€ prompt_eval.py         # ExerciseÂ 5
```

> **Tip:** Reuse `ask_llm()` from LessonÂ 1.1; just import it with:
>
> ```python
> from bootcamp.lesson_1_1_langchain_core.ask_llm import ask_llm
> ```

---

## ğŸ Success Criteria

By the end of this lesson you can:

- Demonstrate each pattern on the command line.
- Explain when to use fewâ€‘shot vs CoT vs ReAct.
- Measure latency & token cost for each pattern.
- Commit code + a short notebook or markdown report summarising findings.

---

## ğŸš€ Commit Message Template

```bash
git add bootcamp/lesson_1_2_prompt_patterns/
git commit -m "feat: lessonÂ 1.2 prompt patterns (fewâ€‘shot, CoT, ReAct)"
```

Letâ€™s prompt like a pro! ğŸ§™â€â™‚ï¸
